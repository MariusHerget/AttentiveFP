{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "### Helper Functions and global settings\n",
    "FORCE_CPU = False\n",
    "line_length = 60\n",
    "def pretty_print_divider(n=1, lb_n=0, char=\"#\"):\n",
    "    if isinstance(n, bool):\n",
    "        n = 1 if n else 0\n",
    "    if lb_n > 0:\n",
    "        print(\"\\n\" * lb_n, end=\"\")\n",
    "    elif n > 1:\n",
    "        print()\n",
    "    for _ in range(n):\n",
    "        print(char * line_length)\n",
    "\n",
    "def pretty_print(message, pb=False, pa=False, lb_n=0, char=\"#\"):\n",
    "    pretty_print_divider(pb, lb_n=lb_n, char=char)\n",
    "    available_space = line_length - 7\n",
    "    formatted_message = f\"{char * 3} {message}\"\n",
    "    padding = line_length - len(formatted_message) - 4\n",
    "    if padding < 0:\n",
    "        formatted_message = formatted_message[:line_length-7] + \"...\"\n",
    "        padding = 0\n",
    "    print(formatted_message + \" \" * padding + f\" {char * 3}\")\n",
    "    pretty_print_divider(pa, char=char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "### multiple GPUs detected (2)                           ###\n",
      "### GPU 0 free memory: 3836477440                        ###\n",
      "### GPU 1 free memory: 5770051584                        ###\n",
      "############################################################\n",
      "### Using device: GPU: cuda:1                            ###\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.manual_seed(8) # for reproduce\n",
    "# Dynamically set the device to use (either GPU or CPU)\n",
    "if torch.cuda.is_available() and not FORCE_CPU:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count > 1:\n",
    "        pretty_print(f\"multiple GPUs detected ({device_count})\", pb=True)\n",
    "        best_device_id = -1\n",
    "        max_free_mem = 0\n",
    "        for i in range(device_count):\n",
    "            free, _ = torch.cuda.mem_get_info(i)\n",
    "            pretty_print(f\"GPU {i} free memory: {free}\")\n",
    "            if free > max_free_mem:\n",
    "                max_free_mem = free\n",
    "                best_device_id = i\n",
    "        device = torch.device(f'cuda:{best_device_id}')\n",
    "        out_device = f\"GPU: cuda:{best_device_id}\"\n",
    "    else:\n",
    "        device = torch.device('cuda')\n",
    "        out_device = \"GPU: cuda:0\"\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    out_device = \"CPU\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.nn.Module.dump_patches = True\n",
    "\n",
    "pretty_print(f\"Using device: {out_device}\", pb=True, pa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "# from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from IPython.display import SVG, display\n",
    "import seaborn as sns; sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### number of all smiles: 2050                           ###\n",
      "### not successfully processed smiles: O=N([O-])C1=C(... ###\n",
      "### not successfully processed smiles: c1(nc(NC(N)=[N... ###\n",
      "### not successfully processed smiles: Cc1nc(sc1)\\[NH... ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:15:57] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] Explicit valence for atom # 11 N, 4, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### not successfully processed smiles: s1cc(CSCCN\\C(N... ###\n",
      "### not successfully processed smiles: c1c(c(ncc1)CSC... ###\n",
      "### not successfully processed smiles: n1c(csc1\\[NH]=... ###\n",
      "### not successfully processed smiles: n1c(csc1\\[NH]=... ###\n",
      "### not successfully processed smiles: n1c(csc1\\[NH]=... ###\n",
      "### not successfully processed smiles: n1c(csc1\\[NH]=... ###\n",
      "### not successfully processed smiles: s1cc(nc1\\[NH]=... ###\n",
      "### not successfully processed smiles: c1(cc(N\\C(=[NH... ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:15:58] Explicit valence for atom # 12 N, 4, is greater than permitted\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### number of successfully processed smiles: 2039        ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:15:58] WARNING: not removing hydrogen atom without neighbors\n",
      "/tmp/ipykernel_342953/2913970869.py:30: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
      "/tmp/ipykernel_342953/2913970869.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "task_name = 'BBBP'\n",
    "tasks = ['BBBP']\n",
    "raw_filename = \"../data/BBBP.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "pretty_print(f\"number of all smiles: {len(smilesList)}\")\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        pretty_print(f\"not successfully processed smiles: {smiles}\")\n",
    "        pass\n",
    "pretty_print(f\"number of successfully processed smiles: {len(remained_smiles)}\")\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "assert canonical_smiles_list[8]==Chem.MolToSmiles(Chem.MolFromSmiles(smiles_tasks_df['cano_smiles'][8]), isomericSmiles=True)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# print(len([i for i in atom_num_dist if i<51]),len([i for i in atom_num_dist if i>50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "random_seed = 188\n",
    "random_seed = int(time.time())\n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "start = time.time()\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 800\n",
    "p_dropout = 0.1\n",
    "fingerprint_dim = 150\n",
    "\n",
    "radius = 3\n",
    "T = 2\n",
    "weight_decay = 2.9 # also known as l2_regularization_lambda\n",
    "learning_rate = 3.5\n",
    "per_task_output_units_num = 2 # for classification model with 2 classes\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:16:01] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBBP</th>\n",
       "      <th>smiles</th>\n",
       "      <th>cano_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [BBBP, smiles, cano_smiles]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smilesList = [smiles for smiles in canonical_smiles_list if len(Chem.MolFromSmiles(smiles).GetAtoms())<101]\n",
    "uncovered = [smiles for smiles in canonical_smiles_list if len(Chem.MolFromSmiles(smiles).GetAtoms())>100]\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[~smiles_tasks_df[\"cano_smiles\"].isin(uncovered)]\n",
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "uncovered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "for i,task in enumerate(tasks):    \n",
    "    negative_df = remained_df[remained_df[task] == 0][[\"smiles\",task]]\n",
    "    positive_df = remained_df[remained_df[task] == 1][[\"smiles\",task]]\n",
    "    weights.append([(positive_df.shape[0]+negative_df.shape[0])/negative_df.shape[0],\\\n",
    "                    (positive_df.shape[0]+negative_df.shape[0])/positive_df.shape[0]])\n",
    "\n",
    "test_df = remained_df.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = remained_df.drop(test_df.index) # training data\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "valid_df.to_csv('sets/valid_df.csv')\n",
    "train_df.to_csv('sets/train_df.csv')\n",
    "test_df.to_csv('sets/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Number of parameters: 649206                         ###\n",
      "### atom_fc.weight torch.Size([150, 39])                 ###\n",
      "### atom_fc.bias torch.Size([150])                       ###\n",
      "### neighbor_fc.weight torch.Size([150, 49])             ###\n",
      "### neighbor_fc.bias torch.Size([150])                   ###\n",
      "### GRUCell.0.weight_ih torch.Size([450, 150])           ###\n",
      "### GRUCell.0.weight_hh torch.Size([450, 150])           ###\n",
      "### GRUCell.0.bias_ih torch.Size([450])                  ###\n",
      "### GRUCell.0.bias_hh torch.Size([450])                  ###\n",
      "### GRUCell.1.weight_ih torch.Size([450, 150])           ###\n",
      "### GRUCell.1.weight_hh torch.Size([450, 150])           ###\n",
      "### GRUCell.1.bias_ih torch.Size([450])                  ###\n",
      "### GRUCell.1.bias_hh torch.Size([450])                  ###\n",
      "### GRUCell.2.weight_ih torch.Size([450, 150])           ###\n",
      "### GRUCell.2.weight_hh torch.Size([450, 150])           ###\n",
      "### GRUCell.2.bias_ih torch.Size([450])                  ###\n",
      "### GRUCell.2.bias_hh torch.Size([450])                  ###\n",
      "### align.0.weight torch.Size([1, 300])                  ###\n",
      "### align.0.bias torch.Size([1])                         ###\n",
      "### align.1.weight torch.Size([1, 300])                  ###\n",
      "### align.1.bias torch.Size([1])                         ###\n",
      "### align.2.weight torch.Size([1, 300])                  ###\n",
      "### align.2.bias torch.Size([1])                         ###\n",
      "### attend.0.weight torch.Size([150, 150])               ###\n",
      "### attend.0.bias torch.Size([150])                      ###\n",
      "### attend.1.weight torch.Size([150, 150])               ###\n",
      "### attend.1.bias torch.Size([150])                      ###\n",
      "### attend.2.weight torch.Size([150, 150])               ###\n",
      "### attend.2.bias torch.Size([150])                      ###\n",
      "### mol_GRUCell.weight_ih torch.Size([450, 150])         ###\n",
      "### mol_GRUCell.weight_hh torch.Size([450, 150])         ###\n",
      "### mol_GRUCell.bias_ih torch.Size([450])                ###\n",
      "### mol_GRUCell.bias_hh torch.Size([450])                ###\n",
      "### mol_align.weight torch.Size([1, 300])                ###\n",
      "### mol_align.bias torch.Size([1])                       ###\n",
      "### mol_attend.weight torch.Size([150, 150])             ###\n",
      "### mol_attend.bias torch.Size([150])                    ###\n",
      "### output.weight torch.Size([2, 150])                   ###\n",
      "### output.bias torch.Size([2])                          ###\n"
     ]
    }
   ],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([smilesList[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "\n",
    "loss_function = [nn.CrossEntropyLoss(torch.tensor(weight),reduction='mean') for weight in weights]\n",
    "model = Fingerprint(radius, T, num_atom_features,num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "model.to(device)\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "pretty_print(f\"Number of parameters: {params}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        pretty_print(f\"{name} {param.data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_function):\n",
    "    model.train()\n",
    "    np.random.seed(epoch)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.tensor(x_atom),\n",
    "                                                 torch.tensor(x_bonds),\n",
    "                                                 torch.tensor(x_atom_index, dtype=torch.long),\n",
    "                                                 torch.tensor(x_bond_index, dtype=torch.long),\n",
    "                                                 torch.tensor(x_mask))\n",
    "#         print(torch.Tensor(x_atom).size(),torch.Tensor(x_bonds).size(),torch.cuda.LongTensor(x_atom_index).size(),torch.cuda.LongTensor(x_bond_index).size(),torch.Tensor(x_mask).size())\n",
    "\n",
    "        model.zero_grad()\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target wrapped in a variable)\n",
    "        loss = 0.0\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "            y_val = batch_df[task].values\n",
    "\n",
    "            validInds = np.where((y_val==0) | (y_val==1))[0]\n",
    "#             validInds = np.where(y_val != -1)[0]\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            y_val_adjust = np.array([y_val[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.tensor(validInds, dtype=torch.long).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "\n",
    "            loss += loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.tensor(y_val_adjust, dtype=torch.long))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "#             print(y_val,y_pred,validInds,y_val_adjust,y_pred_adjust)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    y_val_list = {}\n",
    "    y_pred_list = {}\n",
    "    losses_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, test_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[test_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.tensor(x_atom),\n",
    "                                                 torch.tensor(x_bonds),\n",
    "                                                 torch.tensor(x_atom_index, dtype=torch.long),\n",
    "                                                 torch.tensor(x_bond_index, dtype=torch.long),\n",
    "                                                 torch.tensor(x_mask))\n",
    "        atom_pred = atoms_prediction.data[:,:,1].unsqueeze(2).cpu().numpy()\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "            y_val = batch_df[task].values\n",
    "\n",
    "            validInds = np.where((y_val==0) | (y_val==1))[0]\n",
    "#             validInds = np.where((y_val=='0') | (y_val=='1'))[0]\n",
    "#             print(validInds)\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            y_val_adjust = np.array([y_val[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.tensor(validInds, dtype=torch.long).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "#             print(validInds)\n",
    "            loss = loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.tensor(y_val_adjust, dtype=torch.long))\n",
    "#             print(y_pred_adjust)\n",
    "            y_pred_adjust = F.softmax(y_pred_adjust,dim=-1).data.cpu().numpy()[:,1]\n",
    "            losses_list.append(loss.cpu().detach().numpy())\n",
    "            try:\n",
    "                y_val_list[i].extend(y_val_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "            except:\n",
    "                y_val_list[i] = []\n",
    "                y_pred_list[i] = []\n",
    "                y_val_list[i].extend(y_val_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "#             print(y_val,y_pred,validInds,y_val_adjust,y_pred_adjust)            \n",
    "    test_roc = [roc_auc_score(y_val_list[i], y_pred_list[i]) for i in range(len(tasks))]\n",
    "    test_prc = [auc(precision_recall_curve(y_val_list[i], y_pred_list[i])[1],precision_recall_curve(y_val_list[i], y_pred_list[i])[0]) for i in range(len(tasks))]\n",
    "#     test_prc = auc(recall, precision)\n",
    "    test_precision = [precision_score(y_val_list[i],\n",
    "                                     (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "    test_recall = [recall_score(y_val_list[i],\n",
    "                               (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "    test_loss = np.array(losses_list).mean()\n",
    "\n",
    "    return test_roc, test_prc, test_precision, test_recall, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herget/conda/envs/lea/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:\t0\n",
      "train_roc:[0.6769901056304918]\n",
      "valid_roc:[0.686698717948718]\n",
      "\n",
      "EPOCH:\t1\n",
      "train_roc:[0.7555725047277743]\n",
      "valid_roc:[0.7243589743589743]\n",
      "\n",
      "EPOCH:\t2\n",
      "train_roc:[0.7906175883649738]\n",
      "valid_roc:[0.747596153846154]\n",
      "\n",
      "EPOCH:\t3\n",
      "train_roc:[0.8007355476382025]\n",
      "valid_roc:[0.7530715811965812]\n",
      "\n",
      "EPOCH:\t4\n",
      "train_roc:[0.8108576861593756]\n",
      "valid_roc:[0.7564102564102564]\n",
      "\n",
      "EPOCH:\t5\n",
      "train_roc:[0.8183468984756193]\n",
      "valid_roc:[0.7685630341880343]\n",
      "\n",
      "EPOCH:\t6\n",
      "train_roc:[0.8218783629885802]\n",
      "valid_roc:[0.7620192307692307]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch - best_param[\u001b[33m\"\u001b[39m\u001b[33mroc_epoch\u001b[39m\u001b[33m\"\u001b[39m] >\u001b[32m18\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (epoch - best_param[\u001b[33m\"\u001b[39m\u001b[33mloss_epoch\u001b[39m\u001b[33m\"\u001b[39m] >\u001b[32m28\u001b[39m):        \n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataset, optimizer, loss_function)\u001b[39m\n\u001b[32m     39\u001b[39m             loss += loss_function[i](\n\u001b[32m     40\u001b[39m                 y_pred_adjust,\n\u001b[32m     41\u001b[39m                 torch.tensor(y_val_adjust, dtype=torch.long))\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# Step 5. Do the backward pass and update the gradient\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m#             print(y_val,y_pred,validInds,y_val_adjust,y_pred_adjust)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m         optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/_tensor.py:572\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    529\u001b[39m \n\u001b[32m    530\u001b[39m \u001b[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    569\u001b[39m \u001b[33;03m        used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m torch.autograd.backward(\n\u001b[32m    582\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    583\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/overrides.py:1717\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1714\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1715\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1716\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1717\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1719\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/utils/_device.py:106\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/lea/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_param ={}\n",
    "best_param[\"roc_epoch\"] = 0\n",
    "best_param[\"loss_epoch\"] = 0\n",
    "best_param[\"valid_roc\"] = 0\n",
    "best_param[\"valid_loss\"] = 9e8\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    train_roc, train_prc, train_precision, train_recall, train_loss = eval(model, train_df)\n",
    "    valid_roc, valid_prc, valid_precision, valid_recall, valid_loss = eval(model, valid_df)\n",
    "    train_roc_mean = np.array(train_roc).mean()\n",
    "    valid_roc_mean = np.array(valid_roc).mean()\n",
    "\n",
    "#     tensorboard.add_scalars('ROC',{'train_roc':train_roc_mean,'valid_roc':valid_roc_mean},epoch)\n",
    "#     tensorboard.add_scalars('Losses',{'train_losses':train_loss,'valid_losses':valid_loss},epoch)\n",
    "\n",
    "    if valid_roc_mean > best_param[\"valid_roc\"]:\n",
    "        best_param[\"roc_epoch\"] = epoch\n",
    "        best_param[\"valid_roc\"] = valid_roc_mean\n",
    "        if valid_roc_mean > 0.87:\n",
    "             torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')             \n",
    "    if valid_loss < best_param[\"valid_loss\"]:\n",
    "        best_param[\"loss_epoch\"] = epoch\n",
    "        best_param[\"valid_loss\"] = valid_loss\n",
    "\n",
    "    pretty_print(f\"EPOCH: {epoch}\", pb=3 if epoch == 0 else False)\n",
    "    pretty_print(f\"train_roc: {train_roc}\")\n",
    "    pretty_print(f\"valid_roc: {valid_roc}\", pa=True)\n",
    "\n",
    "    if (epoch - best_param[\"roc_epoch\"] >18) and (epoch - best_param[\"loss_epoch\"] >28):        \n",
    "        break\n",
    "\n",
    "    train(model, train_df, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"roc_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "\n",
    "test_roc, test_prc, test_precision, test_recall, test_losses = eval(best_model, test_df)\n",
    "\n",
    "pretty_print(f\"best epoch: {best_param['roc_epoch']}\", pb=True)\n",
    "pretty_print(f\"test_roc: {test_roc}\")\n",
    "pretty_print(f\"test_roc_mean: {np.array(test_roc).mean()}\", pa=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
